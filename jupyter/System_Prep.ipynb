{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f90f37-d975-49cc-9ff8-e118b25232ae",
   "metadata": {},
   "source": [
    "# System Preparation \n",
    "\n",
    "This notebook serves as notes for reference to getting setup to develop with SUBER.\n",
    "\n",
    "- Your Python virtual environment\n",
    "- GPU Preparation\n",
    "- Pytorch Installation\n",
    "- Install Stable Baselines 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b454da6d-06ac-42d3-8f96-d3977b512d0d",
   "metadata": {},
   "source": [
    "\n",
    "## Your Python Virtual Environments\n",
    "I switched to ordinary Python virtual environments because Anaconda itself was becoming a chore. The Python version used for this project is Python 3.10.12.\n",
    "\n",
    "This creates the initial viriutal environment. I called it sb3 because Stable Baselines 3 is the primary package of the environment.\n",
    "```.bash\n",
    "\n",
    "python -m venv sb3\n",
    "\n",
    "```\n",
    "\n",
    "I also added the following to my .bashrc so that I just type sb3.\n",
    "\n",
    "```.bash\n",
    "\n",
    "alias sb3='source ~/sb3/bin/activate'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9042aca-73bd-4764-a169-56a8188bd4b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GPU Preparation\n",
    "\n",
    "First consider what version of \n",
    "Nvidia GPU driver and nvcc (aka cuda) versions should be within the same major version. I've noticed that Ubuntu 22.04 loads on some systems have been way out of **alignment**. Try to get them at the same version.\n",
    "\n",
    "It is best to try a few things.  I am working with a Windows 11 WSL2 system (Ubuntu 22.04) with 12.6 for the driver and 12.2 for the compiler.  Be ready to troubleshoot.  The example will help you below but alway pay attention to what Pytorch has to say. \n",
    "\n",
    "```\n",
    "sudo apt-get purge 'nvidia*' 'cuda*'\n",
    "\n",
    "sudo apt-get install nvidia-driver-535\n",
    "\n",
    "sudo reboot\n",
    "\n",
    "wget https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda_12.2.0_535.54.03_linux.run\n",
    "\n",
    "chmod a+x cuda_12.2.0_535.54.03_linux.run\n",
    " \n",
    "sudo ./cuda_12.2.0_535.54.03_linux.run # And follow the prompts\n",
    "\n",
    "# Edit your .bashrc and put these in. But don't put the hastags in front of them.\n",
    "# export PATH=/usr/local/cuda-12.2/bin${PATH:+:${PATH}}\n",
    "#export LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n",
    "\n",
    "\n",
    "# I also had to do this.  If you cannot type nvcc --version then you need to check the permissions.\n",
    "sudo chmod -R 755 /usr/local/cuda-12.2\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "The results should be something like this:\n",
    "\n",
    "```\n",
    "acshell@ip-10-114-92-249:~$ nvidia-smi | grep -i \"cuda version\" | awk '{print $9}'\n",
    "12.2\n",
    "acshell@ip-10-114-92-249:~$ nvcc --version\n",
    "nvcc: NVIDIA (R) Cuda compiler driver\n",
    "Copyright (c) 2005-2023 NVIDIA Corporation\n",
    "Built on Tue_Jun_13_19:16:58_PDT_2023\n",
    "Cuda compilation tools, release 12.2, V12.2.91\n",
    "Build cuda_12.2.r12.2/compiler.32965470_0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7795877-6869-4f73-8b4d-200276e90df9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PyTorch Installation\n",
    "\n",
    "You should do this first. If this doesn't work, nothing will. That is to say, if you do not get this installed and tested then nothing will work. \n",
    "\n",
    "PyTorch cuda version should be within a minor version of the cuda drivers and cuda drivers need to align with nvidia drivers.  Try hard to make this happen by paying attention to versions.  \n",
    "\n",
    "```.bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed9f27e-d2b1-4613-8d06-c06b088ecca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Cuda available? True.\n",
      "Torch Cuda Version is 12.1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "print(f\"Is Cuda available? {torch.cuda.is_available()}.\")  # Should return True\n",
    "print(f\"Torch Cuda Version is {torch.version.cuda}.\")  # Should return '12.1'\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc4729f-6f5c-4154-9465-ddc6c117b774",
   "metadata": {},
   "source": [
    "### Torch Examples\n",
    "\n",
    "Here are some examples showing that PyTorch works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc3da4c-9a5b-433e-a7f4-c8a2c6fdfb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication on CPU took: 6.3165 seconds\n",
      "Result tensor size on CPU: torch.Size([10000, 10000])\n",
      "Matrix multiplication on GPU took: 0.1693 seconds\n",
      "Result tensor size on GPU: torch.Size([10000, 10000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Define the size of the tensors\n",
    "size = 10000\n",
    "\n",
    "# Create two large random tensors for CPU\n",
    "tensor1_cpu = torch.randn(size, size)\n",
    "tensor2_cpu = torch.randn(size, size)\n",
    "\n",
    "# Perform matrix multiplication on the CPU and time it\n",
    "start_time = time.time()\n",
    "result_cpu = torch.matmul(tensor1_cpu, tensor2_cpu)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Matrix multiplication on CPU took: {end_time - start_time:.4f} seconds\")\n",
    "print(f\"Result tensor size on CPU: {result_cpu.size()}\")\n",
    "\n",
    "# Check if CUDA is available and perform the same test on the GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    # Create two large random tensors for GPU\n",
    "    tensor1_gpu = tensor1_cpu.to(device)\n",
    "    tensor2_gpu = tensor2_cpu.to(device)\n",
    "\n",
    "    # Perform matrix multiplication on the GPU and time it\n",
    "    torch.cuda.synchronize()  # Ensure all CUDA operations are finished\n",
    "    start_time = time.time()\n",
    "    result_gpu = torch.matmul(tensor1_gpu, tensor2_gpu)\n",
    "    torch.cuda.synchronize()  # Ensure the GPU has finished the computation\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Matrix multiplication on GPU took: {end_time - start_time:.4f} seconds\")\n",
    "    print(f\"Result tensor size on GPU: {result_gpu.size()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71fdeb1-9a02-454c-ac38-1f13c102a7be",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Install Stable Baselines 3\n",
    "\n",
    "```\n",
    "pip install stable-baselines3[extra]\n",
    "\n",
    "```\n",
    "\n",
    "SB3 is the premiere Python module when working with Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef6ed398-a51b-42b5-a509-a1fd94599c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.2\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3\n",
    "print(stable_baselines3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2812c9-78a9-44a2-b997-431bc8498d51",
   "metadata": {},
   "source": [
    "### SB3 Example\n",
    "\n",
    "Note, it takes many iteraitons and the proper algorithm to get good results; this just shows it working.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "066b6955-d608-4996-9f99-cba81995e69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.3     |\n",
      "|    ep_rew_mean     | 22.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 908      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 26.4        |\n",
      "|    ep_rew_mean          | 26.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 783         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009169815 |\n",
      "|    clip_fraction        | 0.0977      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00852    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.7         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 51.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.4        |\n",
      "|    ep_rew_mean          | 34.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009457361 |\n",
      "|    clip_fraction        | 0.0653      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.0887      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 33.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 47.1        |\n",
      "|    ep_rew_mean          | 47.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 761         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009698171 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.626      |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 52.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 62          |\n",
      "|    ep_rew_mean          | 62          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 752         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006239866 |\n",
      "|    clip_fraction        | 0.0465      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.616      |\n",
      "|    explained_variance   | 0.305       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 56.9        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "# Create the CartPole-v1 environment with the \"rgb_array\" render mode\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Create the PPO model (you can replace PPO with other algorithms if you want)\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the agent for 10,000 steps\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Test the trained agent and render in the notebook\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Set up the plot for dynamic updates\n",
    "#plt.ion()  # Turn on interactive mode for matplotlib\n",
    "#fig, ax = plt.subplots()\n",
    "\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "\n",
    "    if done or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a899a604-34d0-4299-be1c-96aaa804e34c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Install Transformers and Tokenizers\n",
    "\n",
    "```\n",
    "pip install -U transformers tokenizers\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8825337-4ecb-483b-aff1-f649ee0d4f07",
   "metadata": {},
   "source": [
    "### Transformer and Tokenizer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b96fc0-bb19-4440-88f6-23db75f85940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Transformers are amazing for NLP tasks.\n",
      "Tokenized Input IDs: tensor([[  101, 19081,  2024,  6429,  2005, 17953,  2361,  8518,  1012,   102]])\n",
      "Decoded Text: transformers are amazing for nlp tasks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acshell/sb3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample text\n",
    "text = \"Transformers are amazing for NLP tasks.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get the tokenized input IDs\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# Decode the token IDs back to text\n",
    "decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print original text, tokenized input, and decoded text\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Tokenized Input IDs:\", input_ids)\n",
    "print(\"Decoded Text:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af7930-6087-4db5-82f9-ae56de146383",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sentence Transformers\n",
    "\n",
    "```/bash\n",
    "pip install sentence-transformers\n",
    "\n",
    "```\n",
    "\n",
    "Need this as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254201e2-3fb5-4f0f-a346-dfed1abb0264",
   "metadata": {},
   "source": [
    "### Sentence Transformers Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62f76f3b-253d-4df7-af11-cac9ee1bd986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.13390070e-02 -2.08391547e-02  3.78195569e-02 -1.00276349e-02\n",
      "  -2.18986608e-02  6.64147083e-03 -4.42283303e-02  4.97135967e-02\n",
      "   2.80647948e-02  1.45602273e-02  2.62472201e-02  8.02669376e-02\n",
      "   4.97585116e-03  8.78880396e-02  4.61270213e-02  3.79769094e-02\n",
      "   3.22095454e-02  1.52603351e-02 -4.78855893e-02 -8.71269032e-02\n",
      "   1.09329306e-01  8.22059587e-02  1.47923771e-02 -5.11702895e-02\n",
      "   5.15239835e-02  6.55859634e-02 -5.36913164e-02 -4.96964194e-02\n",
      "   3.65459099e-02 -9.47065465e-03 -4.13955674e-02  5.72568960e-02\n",
      "  -6.83491826e-02  5.84685206e-02 -6.26850277e-02  7.58528262e-02\n",
      "   1.44350408e-02  1.83785763e-02  1.41708283e-02 -5.39689250e-02\n",
      "  -3.47521193e-02 -1.90681927e-02  1.80511959e-02 -2.25276910e-02\n",
      "   4.55970876e-02 -3.47322971e-02 -2.73609236e-02 -1.98490731e-02\n",
      "  -8.14849976e-04 -2.79270560e-02 -3.95562612e-02 -5.84854893e-02\n",
      "   5.34983836e-02  1.18243903e-01 -1.34978034e-02  2.81568691e-02\n",
      "   8.00181087e-03 -5.52489273e-02  1.07748117e-02 -8.26784372e-02\n",
      "  -7.50691667e-02 -7.26928236e-04 -1.14284651e-02 -3.15912552e-02\n",
      "  -2.56440192e-02 -8.93284567e-03  2.13976782e-02 -1.95716647e-03\n",
      "  -3.98999117e-02  1.71870273e-02 -9.86917391e-02  4.32079509e-02\n",
      "   2.14145463e-02  7.41939768e-02  8.17540959e-02  1.30179999e-02\n",
      "   3.96499448e-02 -1.12352587e-01  1.61440130e-02 -4.75933217e-02\n",
      "   7.10801706e-02 -2.49395724e-02 -1.67709757e-02  1.02371044e-01\n",
      "   6.00531772e-02 -3.79599780e-02 -8.91866162e-04 -1.67397764e-02\n",
      "  -8.11639279e-02 -2.96588079e-03 -8.68672282e-02 -9.48273316e-02\n",
      "   8.75561386e-02  7.51263229e-03 -4.40817252e-02  2.50664987e-02\n",
      "  -4.81076017e-02 -7.26852566e-02  4.49801981e-03  3.30109596e-02\n",
      "  -9.52916034e-03 -2.88514514e-02  2.91740932e-02 -9.59849209e-02\n",
      "  -1.13295622e-01 -7.04405606e-02 -1.06133325e-02  1.11098271e-02\n",
      "  -3.91733460e-03 -4.05783392e-02  3.17186639e-02 -2.89384332e-02\n",
      "  -1.91743225e-02 -4.31948118e-02  6.17150255e-02 -8.93388987e-02\n",
      "   8.42684414e-03 -1.18960850e-02 -2.79276688e-02  3.27752307e-02\n",
      "   1.35773923e-02  7.54369274e-02 -4.16411757e-02  7.63626769e-02\n",
      "   2.60962211e-02 -3.30173448e-02  9.69682336e-02 -2.01159795e-33\n",
      "   8.41613114e-03  9.49250758e-02  4.12810929e-02  3.72696519e-02\n",
      "  -9.30486713e-03  3.43326759e-03 -2.14503668e-02  5.93383163e-02\n",
      "  -3.82257812e-02 -3.54572549e-03 -5.11288680e-02  8.00575390e-02\n",
      "  -1.72844995e-02 -1.36364275e-03 -1.50901964e-02 -6.47914186e-02\n",
      "  -3.30008715e-02  4.24927995e-02 -3.90296653e-02  3.09746135e-02\n",
      "  -6.61235070e-03  4.67417315e-02  3.62695828e-02 -2.47308351e-02\n",
      "  -3.20473835e-02 -3.44661018e-03 -1.98129076e-03 -6.16287068e-02\n",
      "   4.73358110e-02 -1.84988379e-02 -3.48375440e-02  5.97697720e-02\n",
      "  -5.21448851e-02  2.64986157e-02  5.34055801e-03  2.45023379e-03\n",
      "  -2.50871815e-02 -9.50678363e-02 -1.16135553e-02 -1.79991536e-02\n",
      "  -2.73288023e-02  5.74762523e-02 -1.10659629e-01  7.15813180e-03\n",
      "   7.70749897e-02  7.02132359e-02  2.04947311e-03 -4.28030826e-02\n",
      "   2.79471949e-02  1.98840373e-03  3.78896967e-02  5.34533784e-02\n",
      "  -5.07847518e-02  1.97708188e-03  1.83810994e-01 -7.26304390e-03\n",
      "   3.04948669e-02 -4.95860493e-03  1.00949407e-01  2.92117987e-02\n",
      "  -1.23772733e-02  3.00229173e-02  1.59176383e-02 -3.04648578e-02\n",
      "   4.33325842e-02 -2.02232916e-02  5.77009954e-02 -3.25588044e-03\n",
      "  -3.42571497e-04 -1.16551155e-02 -2.14825030e-02 -3.31575871e-02\n",
      "  -8.43170739e-04 -3.40885743e-02  1.86595544e-02  1.45057691e-02\n",
      "   2.44677812e-03 -4.46445048e-02  4.31090854e-02  5.86921908e-02\n",
      "  -4.85014841e-02 -1.19012073e-01  2.97993943e-02 -4.78464365e-02\n",
      "   2.59547047e-02 -4.82602790e-02 -3.18483710e-02 -9.71545726e-02\n",
      "   4.57092887e-03 -9.06090438e-03 -8.83219822e-04  2.50313859e-02\n",
      "  -2.26742737e-02  4.16883966e-03  5.73514402e-03  9.56815667e-34\n",
      "  -3.73278409e-02 -1.25535801e-02 -1.03082284e-01  9.87608209e-02\n",
      "  -2.94563305e-02 -1.43612577e-02 -3.82837169e-02  2.79908068e-02\n",
      "   2.47576591e-02  5.60509004e-02 -8.17170553e-03 -3.16256620e-02\n",
      "  -3.93590704e-02 -2.18429156e-02  1.09793991e-01 -6.70883581e-02\n",
      "   4.63267863e-02 -1.40530290e-02  4.11659032e-02  8.68671909e-02\n",
      "   1.58836525e-02  1.58808187e-01 -1.29776910e-01  4.66321111e-02\n",
      "  -9.48791485e-03  6.40924796e-02 -1.15770943e-01  7.14270212e-03\n",
      "   1.17837517e-02  9.03238915e-03 -4.31012809e-02 -1.03710378e-02\n",
      "  -1.86151527e-02 -2.15608086e-02 -5.08089513e-02 -1.17473854e-02\n",
      "  -2.72203144e-02 -1.23246694e-02 -2.62995996e-02  1.65910013e-02\n",
      "   3.03094927e-02  3.99106108e-02 -2.81111002e-02  2.47760918e-02\n",
      "  -9.52601358e-02 -6.82688504e-02 -3.33690830e-02 -4.25384268e-02\n",
      "  -2.54375045e-03  7.71384090e-02 -3.54083404e-02  1.09947072e-02\n",
      "  -1.12818219e-01 -1.21028423e-01 -1.52590508e-02 -5.44914454e-02\n",
      "   5.40056191e-02 -1.03101231e-01  2.88094450e-02 -1.97712220e-02\n",
      "  -9.30150971e-02  2.82325316e-02  8.50701481e-02 -5.99852428e-02\n",
      "   2.28141919e-02 -3.07087321e-02  1.59088131e-02 -2.78398897e-02\n",
      "  -2.87900819e-03 -2.72179041e-02  6.62282854e-02  3.70688252e-02\n",
      "   2.64302939e-02 -3.94684598e-02 -1.07483650e-02 -1.19047808e-02\n",
      "   4.18200418e-02  7.88502488e-03 -2.53780540e-02 -6.98991567e-02\n",
      "   3.05062011e-02  8.73474404e-03  5.21764979e-02  2.98741274e-02\n",
      "   1.51348971e-02  8.98343399e-02 -1.84008684e-02  1.12341822e-03\n",
      "   3.24808732e-02  6.21275343e-02 -3.42306010e-02  3.46833952e-02\n",
      "   2.67563146e-02  1.13286540e-01  1.31214503e-02 -1.58217190e-08\n",
      "  -5.14111295e-02  5.96944205e-02 -3.73664871e-02 -1.06354561e-02\n",
      "  -1.42139588e-02 -4.84035984e-02  3.31888199e-02  8.29870179e-02\n",
      "  -3.32707316e-02 -3.35283868e-04  9.80553329e-02 -4.55461778e-02\n",
      "   9.90803540e-03  4.91861142e-02  1.17603973e-01  1.87714528e-02\n",
      "   7.86552504e-02 -6.34165341e-03 -3.67030278e-02  8.07172991e-03\n",
      "  -8.98531405e-04  8.72058049e-02 -5.00192419e-02  8.99785571e-03\n",
      "  -2.44703311e-02  2.10707095e-02 -3.14435475e-02 -4.93638217e-02\n",
      "   2.44295467e-02 -1.39967715e-02  3.58096790e-03  4.92439121e-02\n",
      "  -2.20063217e-02  4.35395502e-02  5.47413491e-02  2.43605040e-02\n",
      "   7.40203187e-02 -7.40141198e-02 -1.90981627e-02 -3.42055261e-02\n",
      "   7.88777322e-02  7.65522420e-02 -9.69668627e-02  3.22016552e-02\n",
      "   4.90539335e-02 -2.78661191e-03  4.22286578e-02 -1.70268357e-01\n",
      "  -3.40080447e-02  3.91160417e-03  7.40142632e-03 -2.76137069e-02\n",
      "  -4.57474105e-02  2.54089758e-02  7.90525451e-02  3.73700596e-02\n",
      "   4.22542728e-02 -7.54245818e-02 -5.92881218e-02  7.13606626e-02\n",
      "   1.18064862e-02  7.82025009e-02  1.94492079e-02  4.77532446e-02]\n",
      " [-3.65066994e-03 -4.02566716e-02  5.37926145e-02  1.87592469e-02\n",
      "   4.36548963e-02  1.07265830e-01  1.37051987e-02  5.64765707e-02\n",
      "   3.83178592e-02 -3.81296165e-02  4.78521623e-02  4.42149825e-02\n",
      "   2.23990567e-02  6.06395677e-02  2.15431135e-02  3.58679853e-02\n",
      "   8.68860111e-02  9.80533436e-02 -7.73597658e-02 -8.74792784e-02\n",
      "  -4.10939641e-02  2.51762625e-02  6.71608075e-02 -6.30397201e-02\n",
      "   7.63511052e-03 -9.63680632e-03 -6.72210231e-02  3.96243483e-03\n",
      "   6.95275590e-02  2.14077104e-02 -2.73870700e-03 -4.66553262e-03\n",
      "  -2.08296347e-02  2.78403163e-02 -1.73364673e-02  2.82478556e-02\n",
      "   4.42461483e-02  1.06463179e-01 -4.13178653e-02 -2.04074830e-02\n",
      "   8.36502016e-03  2.78143119e-02  5.13444282e-02  6.12559393e-02\n",
      "   5.86420186e-02 -5.18060364e-02 -6.59484640e-02 -2.18247697e-02\n",
      "  -4.19921130e-02  4.15930487e-02 -6.09701574e-02 -2.82707848e-02\n",
      "  -3.88228372e-02  2.98456158e-02  2.10315306e-02 -6.47069840e-03\n",
      "  -3.32621075e-02 -1.17579261e-02  1.10785477e-02 -9.81175005e-02\n",
      "   1.67629886e-02 -5.59938587e-02 -3.41849029e-02  4.53319177e-02\n",
      "   3.42978686e-02 -6.97385054e-03  3.40199936e-03  5.35050258e-02\n",
      "  -5.67048676e-02  1.18010826e-01 -3.72043252e-02  2.63290983e-02\n",
      "  -8.92801434e-02  3.48124839e-02 -7.75280818e-02  4.94075157e-02\n",
      "   5.68245985e-02 -1.02483295e-01  6.50745556e-02 -2.24783588e-02\n",
      "  -3.21006216e-02 -2.35492010e-02  1.46478750e-02 -1.28405867e-03\n",
      "   1.17351934e-01 -1.12274839e-02  7.39701092e-02 -6.42866194e-02\n",
      "  -6.89552426e-02  2.77736969e-02  8.26349761e-03 -7.05972314e-02\n",
      "   4.64271978e-02 -6.82025822e-03  9.92725696e-03 -1.55898565e-02\n",
      "  -3.29161510e-02 -2.54919436e-02  1.22037521e-02  6.56328574e-02\n",
      "   3.29815373e-02  2.29997355e-02  7.80922472e-02  3.18653905e-03\n",
      "  -4.98883277e-02 -3.59911993e-02 -2.51076091e-02  1.59666073e-02\n",
      "   3.59948981e-03 -7.74266347e-02 -1.08642377e-01  4.89925779e-02\n",
      "  -2.34071631e-02  1.15038678e-02  5.48987389e-02 -2.53492855e-02\n",
      "   1.77409071e-02 -5.13560027e-02  4.09484617e-02 -1.90767348e-02\n",
      "  -2.71398183e-02  5.08160815e-02 -4.04529199e-02  4.74189669e-02\n",
      "   1.87084600e-02 -7.64882788e-02  1.77136324e-02 -3.10000856e-33\n",
      "  -1.21808937e-02  2.59609595e-02 -1.91357657e-02  6.19516782e-02\n",
      "   9.14469920e-03  1.42611191e-02 -4.55985107e-02  6.13452867e-02\n",
      "  -8.51536021e-02 -3.42709720e-02 -3.82091999e-02  5.97606599e-02\n",
      "   1.83344614e-02  1.04790768e-02  5.01671806e-02  1.86543074e-02\n",
      "  -2.31891312e-02  1.37788057e-02  1.65510410e-03  2.48640813e-02\n",
      "  -5.38350195e-02 -1.85126327e-02 -2.82222964e-03 -3.37435976e-02\n",
      "  -3.69313322e-02 -2.38055289e-02  1.20403022e-01 -6.21039830e-02\n",
      "  -4.12108712e-02  2.42503006e-02 -5.59023321e-02 -3.04966662e-02\n",
      "  -6.82482272e-02 -1.79481879e-02  2.20056763e-03  1.10732866e-02\n",
      "   1.02062535e-03 -3.48261595e-02  1.49891833e-02 -8.75050668e-03\n",
      "  -5.40850824e-03 -2.03851014e-02 -9.91616026e-03  2.04790337e-03\n",
      "   1.04808928e-02  1.72011333e-03 -9.23037250e-03 -3.43419239e-02\n",
      "   1.65516827e-02 -4.56563495e-02  1.09758005e-02  2.34714653e-02\n",
      "  -3.89112416e-03 -1.08842529e-01  1.05601937e-01 -2.06439868e-02\n",
      "  -1.60452146e-02  1.41883045e-02  2.73804329e-02 -4.69093807e-02\n",
      "  -1.50368391e-02  1.98541693e-02  9.58696231e-02 -3.54821645e-02\n",
      "  -3.42407934e-02  7.78523833e-02 -5.04241176e-02 -2.30546445e-02\n",
      "   1.91291384e-02  2.52683517e-02 -6.17241822e-02  3.61112282e-02\n",
      "  -2.04768628e-02 -6.49501150e-03 -4.58281189e-02  6.94495365e-02\n",
      "  -5.43259159e-02 -1.00172438e-01  3.08690611e-02  1.17122792e-01\n",
      "  -3.66334780e-03 -1.84244603e-01  1.39171928e-02 -1.40381306e-02\n",
      "  -3.96210067e-02 -1.41388044e-01  5.43646999e-02 -8.71689320e-02\n",
      "   6.75088093e-02  2.89072692e-02 -1.67963840e-02 -9.78565868e-03\n",
      "  -4.55209129e-02 -1.96133950e-03 -2.68604569e-02  1.68950019e-33\n",
      "  -3.31909470e-02  1.94348507e-02 -6.68849126e-02  6.47821426e-02\n",
      "  -8.55552871e-03  2.43361089e-02 -6.32420629e-02  1.61887929e-02\n",
      "  -2.44753286e-02 -1.50398416e-02 -8.92010182e-02  1.34052504e-02\n",
      "   4.28511240e-02 -4.46366072e-02  2.76253391e-02  5.50351338e-03\n",
      "  -4.14503664e-02  2.03786399e-02 -1.62264388e-02  4.22022492e-02\n",
      "   1.41472614e-03  9.94223058e-02 -5.71459755e-02  7.38419741e-02\n",
      "   4.14964594e-02  8.90614763e-02 -6.04642071e-02 -7.20138177e-02\n",
      "  -1.31157190e-01 -5.55766895e-02 -3.43460813e-02 -4.77558486e-02\n",
      "  -3.85389253e-02 -4.77392338e-02 -5.84639944e-02  3.23132202e-02\n",
      "   7.33683184e-02 -4.08573709e-02 -1.83145292e-02 -3.50779444e-02\n",
      "   5.94206415e-02  5.31809181e-02 -1.55665837e-02  2.80934572e-02\n",
      "  -1.04945684e-02 -1.43888304e-02 -7.55967945e-02 -1.17197633e-02\n",
      "   7.57965669e-02  5.81873767e-02 -4.25755791e-02  3.81851494e-02\n",
      "  -8.37945268e-02 -5.61070852e-02 -1.72014702e-02 -5.63367307e-02\n",
      "   2.60048895e-03 -1.02772839e-01 -1.26568461e-03 -1.56447850e-02\n",
      "  -1.09514229e-01 -3.83286620e-03  3.97503376e-02 -4.62057479e-02\n",
      "   5.93802258e-02 -1.39045328e-01 -3.16912755e-02  6.14067614e-02\n",
      "  -2.19795341e-03 -9.27408114e-02  1.07508423e-02 -3.91478417e-03\n",
      "  -3.00360676e-02  5.58332987e-02 -3.05787660e-02 -2.35126130e-02\n",
      "   3.72293219e-02  1.56597421e-02 -6.71998113e-02 -6.47744089e-02\n",
      "   8.87671709e-02 -7.48190805e-02  6.93876371e-02 -3.46382782e-02\n",
      "   3.34317684e-02  7.62093961e-02  3.34793702e-02 -2.96908664e-03\n",
      "   2.07741521e-02 -8.93890765e-03 -6.33103102e-02 -1.24690523e-02\n",
      "  -1.75032094e-02  1.17574796e-01  3.06493193e-02 -1.57106008e-08\n",
      "  -6.15324266e-02 -2.43087746e-02  5.48808947e-02  3.30633000e-02\n",
      "  -7.56444559e-02 -7.11502135e-02 -2.29836609e-02  6.43548518e-02\n",
      "  -5.09497449e-02 -3.36610014e-03  5.79673201e-02  2.19356269e-02\n",
      "  -6.35055080e-02  1.38764456e-02  4.74086143e-02  4.33154404e-02\n",
      "   5.74724237e-03 -1.79616064e-02 -3.80588532e-03  2.26670448e-02\n",
      "   5.74051179e-02  1.11191250e-01 -6.37347670e-03  4.52824868e-02\n",
      "  -1.78189091e-02  3.89134586e-02  1.69905666e-02  3.40934284e-02\n",
      "   2.54978426e-02  2.16917433e-02  5.06390408e-02  8.18910301e-02\n",
      "  -9.80570260e-03  4.50543361e-03  4.65276837e-02  1.29042089e-01\n",
      "   7.15280101e-02 -1.00725010e-01 -2.45623552e-04 -3.17353383e-03\n",
      "   5.04630320e-02  6.68728277e-02 -8.23084116e-02  3.21189463e-02\n",
      "   1.00300260e-01  4.13455674e-03  2.14019045e-02 -4.83180135e-02\n",
      "  -4.03572991e-02  3.44823976e-03  3.83812599e-02 -5.31095862e-02\n",
      "   5.30554308e-03  1.54014518e-02  3.79170477e-02  3.94545905e-02\n",
      "   3.25820819e-02 -2.72934139e-02 -5.38102910e-03  3.48102935e-02\n",
      "  -9.61133465e-03  1.74200460e-01  3.71564329e-02 -4.88811405e-03]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acshell/sb3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode a list of sentences\n",
    "sentences = [\"Transformers are amazing for NLP tasks.\", \"Sentence embeddings are useful.\"]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Print the sentence embeddings\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe4263a-5b09-4c82-9b14-77d8eefc3e3a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Other Stuff\n",
    "\n",
    "If you are using Jupyter notebook, be sure to pip install `jupyterlab` and `ipywidgets` with pip.\n",
    "\n",
    "```.bash\n",
    "\n",
    "pip install jupyterlab\n",
    "\n",
    "pip install ipywidgets # for tqdm to work properly\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8975939-dfa8-4f6a-8835-08377eae50b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3727ee5a-377c-42ed-95f0-01cc268afd01",
   "metadata": {},
   "source": [
    "## Setup Summary -- \n",
    "\n",
    "More notes can be added here but Pytorch, and Stable Baselines 3 are the two main modules.  Extras required from both will come up but should not be a huge issue.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
