{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d8cb1d-899a-49d8-a041-cf216e8bb186",
   "metadata": {},
   "source": [
    "# Getting Started with SUBER\n",
    "\n",
    "SUBER is complex so we will take it one module at a time. The entry point for suber is [algorithms.mind.CFTrain2](https://github.com/acsheller/SUBER/blob/main/algorithms/mind/CF_train_A2C2.py).  One can review this file and simply get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "531f245e-fadd-4903-affa-c71011f9a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bcaf465-afeb-47cc-a942-9084f4a1c0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/asheller/SUBERX'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064ab627-c241-4237-8a47-cf62e18bbaf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'environment.movies'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableBaselineWrapperNum\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmind\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_enviroment_from_args, get_base_parser\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_LLM\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_logger\n",
      "File \u001b[0;32m~/SUBERX/environment/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Simulatio4RecSys\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLLM\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_LLM\n",
      "File \u001b[0;32m~/SUBERX/environment/env.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mitem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ItemsLoader\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Will need a special one for MIND\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mitems_retrieval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ItemsRetrieval, ItemsRetrievalMind\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mitems_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ItemsSelector,ItemsSelectorMind\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLLM\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMRater\n",
      "File \u001b[0;32m~/SUBERX/environment/items_retrieval.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmovies\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmovie\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Movie\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmind\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnews\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m News\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserMovieInteraction, UserNewsInteraction\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'environment.movies'"
     ]
    }
   ],
   "source": [
    "from algorithms.wrappers import StableBaselineWrapperNum\n",
    "from environment.mind.configs import get_enviroment_from_args, get_base_parser\n",
    "from environment import load_LLM\n",
    "from algorithms.logging_config import get_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1d23b6-fdaa-4e58-bcee-978b0d631de4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'algorithms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmonitor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Monitor\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableBaselineWrapperNum\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmind\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_enviroment_from_args, get_base_parser\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_LLM\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'algorithms'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "from stable_baselines3.common.distributions import CategoricalDistribution\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.preprocessing import get_flattened_obs_dim\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.type_aliases import TensorDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from typing import Callable, Tuple\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import argparse\n",
    "from algorithms.wrappers import StableBaselineWrapperNum\n",
    "from environment.mind.configs import get_enviroment_from_args, get_base_parser\n",
    "from environment import load_LLM\n",
    "from algorithms.logging_config import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63e041-f180-4093-b3d6-fdafaab497f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5dee5a-fcd7-4692-a795-14a13d4eeea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "from stable_baselines3.common.distributions import CategoricalDistribution\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.preprocessing import get_flattened_obs_dim\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.type_aliases import TensorDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from typing import Callable, Tuple\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "import wandb\n",
    "import argparse\n",
    "from algorithms.wrappers import StableBaselineWrapperNum\n",
    "from environment.mind.configs import get_enviroment_from_args, get_base_parser\n",
    "from environment import load_LLM\n",
    "from algorithms.logging_config import get_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff8f9f8-ab82-43ed-bf5c-07003dabcbde",
   "metadata": {},
   "source": [
    "## Preliminary Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e9e98-0a45-4f54-beb2-d495b3d4dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(\"suber_logger\")\n",
    "\n",
    "# Set environment variable to avoid tokenizer parallelism issues\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Define arguments\n",
    "def parse_args():\n",
    "    '''\n",
    "    This parses and adds default arguments to the system.  \n",
    "    '''\n",
    "    parser = get_base_parser()\n",
    "    parser.add_argument(\"--model-device\", type=str, default=\"cuda:0\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.975)\n",
    "    parser.add_argument(\"--embedding-dim\", type=int, default=32)\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=0.01)\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--news-dataset', \n",
    "        choices=['mind_dataset', 'small_mind_dataset'], \n",
    "        help='Specify the news dataset to use',\n",
    "        default='small_mind_dataset'\n",
    "    )\n",
    "    # TODO Parser arguments should be here -- need to consider as most of them are \n",
    "    # in config.py\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "# Linear schedule function\n",
    "def linear_schedule(initial_value: float):\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        current_value = initial_value * progress_remaining\n",
    "        #print(f\"Linear schedule called: progress_remaining={progress_remaining}, learning_rate={current_value}\")\n",
    "        #logger.info(f\"Linear schedule called: progress_remaining={progress_remaining}, learning_rate={current_value}\")\n",
    "        return current_value\n",
    "    return func\n",
    "\n",
    "class CombinedCallback(BaseCallback):\n",
    "    def __init__(self, save_freq=50000, log_freq=5000, save_path=\"./tmp/models/\", name_prefix=\"rl_model\", verbose=0):\n",
    "        super(CombinedCallback, self).__init__(verbose)\n",
    "        self.save_freq = save_freq\n",
    "        self.log_freq = log_freq\n",
    "        self.save_path = save_path\n",
    "        self.name_prefix = name_prefix\n",
    "        self.metrics = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.log_freq == 0:\n",
    "            rewards = self.locals['rewards']\n",
    "            episode_length = self.locals.get('episode_lengths', None)\n",
    "            value_loss = self.locals.get('value_loss', None)\n",
    "            policy_loss = self.locals.get('policy_loss', None)\n",
    "            if value_loss is not None:\n",
    "                log_message = {\n",
    "                    \"step\": self.num_timesteps,\n",
    "                    \"reward\": rewards,\n",
    "                    \"episode_length\": episode_length,\n",
    "                    \"value_loss\": value_loss,\n",
    "                    \"policy_loss\": policy_loss\n",
    "                }\n",
    "                self.metrics.append(log_message)\n",
    "                self.logger.info(log_message)\n",
    "\n",
    "\n",
    "        if self.n_calls % self.save_freq == 0:\n",
    "            model_path = f\"{self.save_path}/{self.name_prefix}_{self.num_timesteps}_steps\"\n",
    "            self.model.save(model_path)\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Saving model checkpoint to {model_path}\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca3018-79b0-4a4b-a41e-20672378f981",
   "metadata": {},
   "source": [
    "# SUBER - Simulated User Behavior for Recommender Systems\n",
    "\n",
    "This is a project from the following Github repository\n",
    "\n",
    "```\n",
    "https://github.com/acsheller/SUBER\n",
    "```\n",
    "\n",
    "\n",
    "Which is derived from \n",
    "\n",
    "```\n",
    "https://github.com/SUBER-Team/SUBER\n",
    "```\n",
    "\n",
    "Here is the help for the code.\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "python3 -m algorithms.mind.CF_train_A2C2 --help\n",
    "\n",
    "usage: CF_train_A2C.py [-h]\n",
    "                       [--llm-model {TheBloke/Llama-2-7b-Chat-GPTQ,TheBloke/Llama-2-13B-chat-GPTQ,TheBloke/vicuna-7B-v1.3-GPTQ,TheBloke/vicuna-13b-v1.3.0-GPTQ,TheBloke/vicuna-33B-GPTQ,TheBloke/vicuna-7B-v1.5-GPTQ,TheBloke/vicuna-13B-v1.5-GPTQ,gpt-3.5-turbo-0613,gpt-4-0613,gpt-4o,TheBloke/Mistral-7B-Instruct-v0.2-GPTQ}]\n",
    "                       [--llm-rater {2Shot_system_our,1Shot_system_our,0Shot_system_our,0Shot_system_our_1_10,1Shot_system_our_1_10,2Shot_system_our_1_10,2Shot_system_our_one_ten,1Shot_system_our_one_ten,2Shot_invert_system_our,1Shot_invert_system_our}]\n",
    "                       [--items-retrieval {last_3,most_similar_3_title,most_similar_3_abstract,none,simple_3}] [--user-dataset {mind}] [--perturbator {none,gaussian,greedy}]\n",
    "                       [--reward-shaping {identity,exp_decay_time,random_watch,same_film_terminate}] [--seed SEED] [--model-device MODEL_DEVICE] [--gamma GAMMA] [--embedding-dim EMBEDDING_DIM]\n",
    "                       [--learning_rate LEARNING_RATE]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --llm-model {TheBloke/Llama-2-7b-Chat-GPTQ,TheBloke/Llama-2-13B-chat-GPTQ,TheBloke/vicuna-7B-v1.3-GPTQ,TheBloke/vicuna-13b-v1.3.0-GPTQ,TheBloke/vicuna-33B-GPTQ,TheBloke/vicuna-7B-v1.5-GPTQ,TheBloke/vicuna-13B-v1.5-GPTQ,gpt-3.5-turbo-0613,gpt-4-0613,gpt-4o,TheBloke/Mistral-7B-Instruct-v0.2-GPTQ}\n",
    "  --llm-rater {2Shot_system_our,1Shot_system_our,0Shot_system_our,0Shot_system_our_1_10,1Shot_system_our_1_10,2Shot_system_our_1_10,2Shot_system_our_one_ten,1Shot_system_our_one_ten,2Shot_invert_system_our,1Shot_invert_system_our}\n",
    "  --items-retrieval {last_3,most_similar_3_title,most_similar_3_abstract,none,simple_3}\n",
    "  --user-dataset {mind}\n",
    "  --perturbator {none,gaussian,greedy}\n",
    "  --reward-shaping {identity,exp_decay_time,random_watch,same_film_terminate}\n",
    "  --seed SEED\n",
    "  --model-device MODEL_DEVICE\n",
    "  --gamma GAMMA\n",
    "  --embedding-dim EMBEDDING_DIM\n",
    "  --learning_rate LEARNING_RATE\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a9beb-7e9b-4516-b9f0-c8ad4813ad25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db70d8-99db-41f7-b5b8-05503201bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_space: gym.spaces.Space, num_users: int, num_items: int, learning_rate: float = 0.001):\n",
    "        super().__init__()\n",
    "        embedding_dim = args.embedding_dim\n",
    "        self.latent_dim_pi = embedding_dim * 2\n",
    "        self.latent_dim_vf = embedding_dim * 2\n",
    "\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(self.user_embedding.embedding_dim, embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim * 2, embedding_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim * 4, num_items)\n",
    "        )\n",
    "\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(self.user_embedding.embedding_dim + num_items, self.latent_dim_vf * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.latent_dim_vf * 2, embedding_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim * 4, self.latent_dim_vf),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, features: TensorDict) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        user_id = features[\"user_id\"].squeeze(1)\n",
    "        news_seen = features[\"items_interact\"]\n",
    "\n",
    "        user_embedding = self.user_embedding(user_id)\n",
    "        user_embedding_value = torch.cat([user_embedding, news_seen], dim=1)\n",
    "        user_bias = self.user_bias(user_id)\n",
    "\n",
    "        mask = features[\"items_interact\"].to(dtype=torch.bool)\n",
    "        logits = self.policy_net(user_embedding) + user_bias\n",
    "        logits[mask] = -torch.inf\n",
    "        return logits, self.value_net(user_embedding_value)\n",
    "\n",
    "    def forward_actor(self, features: TensorDict) -> torch.Tensor:\n",
    "        user_id = features[\"user_id\"].squeeze(1)\n",
    "        user_embedding = self.user_embedding(user_id)\n",
    "        user_bias = self.user_bias(user_id)\n",
    "\n",
    "        mask = features[\"items_interact\"].to(dtype=torch.bool)\n",
    "        logits = self.policy_net(user_embedding) + user_bias\n",
    "        logits[mask] = -torch.inf\n",
    "        return logits\n",
    "\n",
    "    def forward_critic(self, features: TensorDict) -> torch.Tensor:\n",
    "        user_id = features[\"user_id\"].squeeze(1)\n",
    "        news_seen = features[\"items_interact\"]\n",
    "\n",
    "        user_embedding = self.user_embedding(user_id)\n",
    "        user_embedding_value = torch.cat([user_embedding, news_seen], dim=1)\n",
    "        return self.value_net(user_embedding_value)\n",
    "\n",
    "class DistributionUseLogitsDirectly(CategoricalDistribution):\n",
    "    def __init__(self, action_dim: int):\n",
    "        super().__init__(action_dim)\n",
    "\n",
    "    def proba_distribution_net(self, latent_dim: int) -> nn.Module:\n",
    "        return nn.Identity(latent_dim)\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, observation_space: spaces.Space, action_space: spaces.Space, lr_schedule: Callable[[float], float], *args, **kwargs):\n",
    "        kwargs[\"ortho_init\"] = True\n",
    "        super().__init__(observation_space, action_space, lr_schedule, *args, **kwargs)\n",
    "\n",
    "        self.action_dist = DistributionUseLogitsDirectly(action_space.n)\n",
    "        self._build(lr_schedule)\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        default_lr = 0.01\n",
    "        num_users = train_env.get_wrapper_attr('num_users')\n",
    "        num_items = train_env.get_wrapper_attr('num_items')\n",
    "        self.mlp_extractor = Net(self.observation_space, num_users, num_items, learning_rate=default_lr)\n",
    "\n",
    "class ExtractPass(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space) -> None:\n",
    "        super().__init__(observation_space, get_flattened_obs_dim(observation_space))\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        observations[\"user_id\"] = observations[\"user_id\"].int()\n",
    "        return observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8624771a-cff1-4481-867b-e9d899049e99",
   "metadata": {},
   "source": [
    "## This is an example call to the system.  \n",
    "\n",
    "python3 -m algorithms.mind.CF_train_A2C2 --llm-model=TheBloke/Llama-2-13B-chat-GPTQ --llm-rater=2Shot_system_our --perturbator=gaussian --items-retrieval=most_similar_3_title --reward-shaping=exp_decay_time --embedding-dim=512 --gamma=0.95 --seed=42 --news-dataset=mind_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e3aaa3-aa4a-42cc-bd7e-06d98d6f299c",
   "metadata": {},
   "source": [
    "### Set Environment Variables\n",
    "\n",
    "Set these to whatever available option you would like to try out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff8f5a-0b22-42da-a90f-8d907c2d7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {}\n",
    "model_params[\"llm_model\"]       = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "model_params[\"llm_rater\"]       = \"2Shot_system_our\"\n",
    "model_params[\"items_retrieval\"] = \"most_similar_3_title\"\n",
    "model_params[\"news_dataset\"]    = \"small_news_dataset\"\n",
    "model_params[\"perturbator\"]     = \"gaussian\"\n",
    "model_params[\"reward_shaping\"]  = \"exp_decay_time\"\n",
    "model_params[\"seed\"]            = 42\n",
    "model_params[\"model_device\"]    = \"cuda:0\"\n",
    "model_params[\"gamma\"]           = 0.95\n",
    "model_params[\"embedding_dim\"]   = 512\n",
    "\n",
    "args = argparse.Namespace(**model_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de79b17-d7b7-471f-affd-df1dc990d98f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d26e9d-cf9d-451f-b2fd-42fc720af63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "llm = load_LLM(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f88de-b0b5-46f1-9353-9840ed615374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124ea68-30a8-43cf-b3ad-8aa78940450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    dir_name = f\"{args.llm_model}___{args.llm_rater}___{args.items_retrieval}___{args.user_dataset}___{args.news_dataset}___{args.perturbator}___{args.reward_shaping}___{args.seed}___{args.model_device}___{args.gamma}___{args.embedding_dim}___{args.learning_rate}\"\n",
    "    sanitized_dir_name = dir_name.replace('/', '_').replace(':', '_').replace('.', '_')\n",
    "    save_path = f\"./tmp/models/{sanitized_dir_name}\"\n",
    "    wandb_path = f\"./tmp/wandb\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260916d4-f413-4833-85ed-9416856f6220",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    llm-model = TheBloke/Llama-2-7b-Chat-GPTQ\n",
    "    args = parse_args()\n",
    "    llm = load_LLM(args.llm_model)\n",
    "\n",
    "    dir_name = f\"{args.llm_model}_{args.llm_rater}_{args.items_retrieval}_{args.user_dataset}_{args.news_dataset}_{args.perturbator}_{args.reward_shaping}_{args.seed}_{args.model_device}_{args.gamma}_{args.embedding_dim}_{args.learning_rate}\"\n",
    "    sanitized_dir_name = dir_name.replace('/', '_').replace(':', '_').replace('.', '_')\n",
    "    save_path = f\"./tmp/models/{sanitized_dir_name}\"\n",
    "    wandb_path = f\"./tmp/wandb\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    train_env = get_enviroment_from_args(llm, args)\n",
    "    test_env = get_enviroment_from_args(llm, args, seed=args.seed + 600)\n",
    "\n",
    "    policy_kwargs = dict(features_extractor_class=ExtractPass)\n",
    "    train_env = StableBaselineWrapperNum(train_env)\n",
    "    test_env = Monitor(StableBaselineWrapperNum(test_env))\n",
    "\n",
    "    check_env(train_env)\n",
    "    check_env(test_env)\n",
    "\n",
    "    model = A2C(\n",
    "        CustomActorCriticPolicy,\n",
    "        train_env,\n",
    "        verbose=1,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        device=args.model_device,\n",
    "        learning_rate=linear_schedule(args.learning_rate),\n",
    "        tensorboard_log=save_path,\n",
    "        gamma=args.gamma,\n",
    "        ent_coef=0.001,\n",
    "    )\n",
    "\n",
    "    combined_callback = CombinedCallback(save_freq=2500, log_freq=500, save_path=save_path, name_prefix=\"rl_model\", verbose=1)\n",
    "    callback = CallbackList([combined_callback])\n",
    "\n",
    "    logger.info(\"Model starts learning\")\n",
    "\n",
    "    model.learn(total_timesteps=10000, progress_bar=True, callback=callback, tb_log_name=\"t_logs\")\n",
    "\n",
    "    logger.info(\"Model Ends Learning\")\n",
    "    \n",
    "    logger.info(\"Evaluating the Policy\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, test_env, n_eval_episodes=50)\n",
    "    logger.info(f\"Mean reward: {mean_reward} +/- {std_reward}\")\n",
    "\n",
    "    reward_file_path = os.path.join(save_path, f\"reward_{mean_reward:.2f}.txt\")\n",
    "    with open(reward_file_path, 'w') as file:\n",
    "        file.write(f\"Mean reward: {mean_reward} +/- {std_reward}\\n\")\n",
    "\n",
    "    print(f\"Reward information saved to {reward_file_path}\")\n",
    "\n",
    "    print(f\"Mean Reward: {mean_reward} +/- {std_reward}\")\n",
    "    logger.info(f\"Mean Reward: {mean_reward} +/- {std_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada295f-4c90-4f42-a1c9-b7e0be03cdae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
